Subject: [PATCH] destroy_llm_model stop_response_llm_model
---
Index: include/models/basellm.h
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/include/models/basellm.h b/include/models/basellm.h
--- a/include/models/basellm.h	(revision fd8abf5daaeb1e9c2948bff9cb0f3cc3db86436a)
+++ b/include/models/basellm.h	(revision 4b2c88c9485eba90854b361eed1641f73ba39784)
@@ -17,6 +17,7 @@
 namespace fastllm {
     struct ResponseContext {
         bool isEnding = false;
+        bool toStop = false;
         std::vector <std::pair <Data, Data> > pastKeyValues;
         std::vector <int> currentTokens;
         std::queue <int> resultTokenQueue;
@@ -105,6 +106,8 @@
         virtual int LaunchResponseTokens(const std::vector <int> &inputTokens,
                                          const GenerationConfig &generationConfig = GenerationConfig()); // 启动一个response任务，返回分配的handleId
 
+        virtual int StopResponseTokens(int handleId); // 停止指定handle的response任务，-1代表任务不存在
+
         virtual int FetchResponseTokens(int handleId); // 获取指定handle的输出, -1代表输出结束了
 
         virtual int FetchResponseLogits(int handleId, std::vector <float> &logits); // 获取指定handle的输出Logits
Index: src/models/basellm.cpp
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/src/models/basellm.cpp b/src/models/basellm.cpp
--- a/src/models/basellm.cpp	(revision fd8abf5daaeb1e9c2948bff9cb0f3cc3db86436a)
+++ b/src/models/basellm.cpp	(revision 4b2c88c9485eba90854b361eed1641f73ba39784)
@@ -13,8 +13,10 @@
 namespace fastllm {
     int ResponseContextDict::CreateHandle() {
         locker.lock();
-        int newId = 0;
+        int newId = std::chrono::duration_cast<std::chrono::milliseconds>(
+                std::chrono::system_clock::now().time_since_epoch()).count();
         while (dicts.find(newId) != dicts.end()) {
+            std::cout << "handle id conflicting!!!" << std::endl;
             newId++;
         }
         dicts[newId] = new ResponseContext();
@@ -344,7 +346,8 @@
                                 continue;
                             }
                             for (auto &it: model->responseContextDict.dicts) {
-                                if (it.second->isEnding) {
+                                if (it.second->isEnding || it.second->toStop) {
+                                    it.second->isEnding = true;
                                     continue;
                                 }
                                 if (isPrompt && it.second->preTokens != 0) {
@@ -467,31 +470,39 @@
         return handleId;
     }
 
+    int basellm::StopResponseTokens(int handleId) {
+        int ret = -1;
+        dictLocker.lock();
+        ResponseContext *context = responseContextDict.GetHandle(handleId);
+        if (context != nullptr) {
+            context->toStop = true;
+            ret = 0;
+        }
+        dictLocker.unlock();
+        return ret;
+    }
+
     int basellm::FetchResponseTokens(int handleId) {
-        dictLocker.lock();
-        ResponseContext *context = responseContextDict.GetHandle(handleId);
-        if (context == nullptr) {
-            dictLocker.unlock();
-            return -1;
-        } else {
-            while (true) {
-                if (context->resultTokenQueue.size() > 0) {
-                    int ret = context->resultTokenQueue.front();
-                    context->resultTokenQueue.pop();
-                    dictLocker.unlock();
-                    return ret;
-                } else {
-                    if (context->isEnding) {
-                        responseContextDict.RemoveHandle(handleId);
-                        dictLocker.unlock();
-                        return -1;
-                    }
-                }
-                dictLocker.unlock();
-                MySleep(0);
-                dictLocker.lock();
-            }
+        while (true) {
+            dictLocker.lock();
+            ResponseContext *context = responseContextDict.GetHandle(handleId);
+            if (context == nullptr) {
+                dictLocker.unlock();
+                return -1;
+            } else if (!context->resultTokenQueue.empty()) {
+                int ret = context->resultTokenQueue.front();
+                context->resultTokenQueue.pop();
+                dictLocker.unlock();
+                return ret;
+            } else if (context->isEnding) {
+                responseContextDict.RemoveHandle(handleId);
+                dictLocker.unlock();
+                return -1;
+            }
+            dictLocker.unlock();
+            MySleep(0);
         }
+        return -1;
     }
 
     int basellm::FetchResponseLogits(int handleId, std::vector<float> &logits) {
Index: tools/fastllm_pytools/llm.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/tools/fastllm_pytools/llm.py b/tools/fastllm_pytools/llm.py
--- a/tools/fastllm_pytools/llm.py	(revision fd8abf5daaeb1e9c2948bff9cb0f3cc3db86436a)
+++ b/tools/fastllm_pytools/llm.py	(revision 4b2c88c9485eba90854b361eed1641f73ba39784)
@@ -11,11 +11,16 @@
 fastllm_lib.create_llm_model.argtypes = [ctypes.c_char_p]
 fastllm_lib.create_llm_model.restype = ctypes.c_int
 
+fastllm_lib.destroy_llm_model.argtypes = [ctypes.c_int]
+
 fastllm_lib.launch_response_llm_model.argtypes = [ctypes.c_int, ctypes.c_int, ctypes.c_void_p,
                                                   ctypes.c_int, ctypes.c_bool, ctypes.c_float, ctypes.c_int,
                                                   ctypes.c_float, ctypes.c_float, ctypes.c_bool]
 fastllm_lib.launch_response_llm_model.restype = ctypes.c_int
 
+fastllm_lib.stop_response_llm_model.argtypes = [ctypes.c_int, ctypes.c_int]
+fastllm_lib.stop_response_llm_model.restype = ctypes.c_int
+
 fastllm_lib.fetch_response_llm_model.argtypes = [ctypes.c_int, ctypes.c_int]
 fastllm_lib.fetch_response_llm_model.restype = ctypes.c_int
 
Index: tools/src/pytools.cpp
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/tools/src/pytools.cpp b/tools/src/pytools.cpp
--- a/tools/src/pytools.cpp	(revision fd8abf5daaeb1e9c2948bff9cb0f3cc3db86436a)
+++ b/tools/src/pytools.cpp	(revision 4b2c88c9485eba90854b361eed1641f73ba39784)
@@ -60,10 +60,18 @@
 
         fastllm::basellm *GetModel(int handle) {
             locker.lock();
-            auto ret = models[handle].get();
+            auto ret = models.find(handle) != models.end() ? models[handle].get() : nullptr;
             locker.unlock();
             return ret;
         }
+
+        void removeModel(int handle) {
+            locker.lock();
+            if (models.find(handle) != models.end()) {
+                models.erase(handle);
+            }
+            locker.unlock();
+        }
     };
 
     static ModelManager models;
@@ -105,6 +113,10 @@
         return id;
     }
 
+    DLL_EXPORT void destroy_llm_model(int modelId) {
+        models.removeModel(modelId);
+    }
+
     DLL_EXPORT int get_tokenizer_vocab_size(int modelId) {
         auto model = models.GetModel(modelId);
         int ret = model->weight.tokenizer.tokenToStringDict.size();
@@ -234,12 +246,17 @@
         }
         auto config = make_config(max_length, do_sample, top_p, top_k, temperature, repeat_penalty, output_logits);
         auto model = models.GetModel(modelId);
-        return model->LaunchResponseTokens(input, config);
+        return model == nullptr ? -1 : model->LaunchResponseTokens(input, config);
+    }
+
+    DLL_EXPORT int stop_response_llm_model(int modelId, int handleId) {
+        auto model = models.GetModel(modelId);
+        return model == nullptr ? -1 : model->StopResponseTokens(handleId);
     }
 
     DLL_EXPORT int fetch_response_llm_model(int modelId, int handleId) {
         auto model = models.GetModel(modelId);
-        return model->FetchResponseTokens(handleId);
+        return model == nullptr ? -1 : model->FetchResponseTokens(handleId);
     }
 
     DLL_EXPORT int fetch_response_logits_llm_model(int modelId, int handleId, float *logits) {
